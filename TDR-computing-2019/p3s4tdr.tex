\documentclass[pdftex,12pt,letter]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{cite}
\usepackage{url}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}


\newcommand{\pd}{protoDUNE\xspace}
\newcommand{\filesize}{8\,GB\xspace}
\begin{document}
%
\title{Supporting Materials for the DUNE TDR Computing sections on the Prompt Processing System and Data Quality Monitoring in \pd-SP and DUNE in general}


\author{M.\,Potekhin}

%
\maketitle
%
\section{Motivations for p3s and DQM}
\label{sec:motivations}
%The DUNE Collaboration has developed and utilized a prompt processing system to support
%the Data Quality Monitoring activities crucial to the commissioning and operations of
%\pd-SP.


The \pd-SP Data Acquisition (DAQ) is equipped with a capable online monitoring system
which receives data via the network and does processing in real time.
However, certain types of processing required for Data Quality Monitoring (DQM), 
require more resources than are available within the DAQ footprint, and include jobs
that take substantially longer time than processes run in the online monitor.
That could present a resource and data management problem for the DAQ and
intefere with the crucial DAQ function. We note that it is necessary to insulate the DAQ
from frequent software updates of the DQM software dictated by the dynamic nature comissioning
and operations of the experiment. 
For these reasons a separate \textit{\pd Prompt Processing System} (abbreviated as p3s) was put
in place  with the following characteristics \cite{eps}:
\begin{itemize} 

\item turnaround time on the scale of minutes (in limited cases tens of minutes)

\item processing a small fraction of events compared  to the online monitor
but in more detail and with a variety of methods

\item extensive and scalable computing resources provided by the CERN batch facility

\item accessing data in CERN central storage, with no direct coupling to the DAQ system

\item considerable flexibility in introducing, modifying and configuring software 

\item substantial storage and browsing capability for accessing the DQM results

\end{itemize}


\section{The DQM Content}
The following types of DQM applications were used at various times during
commissioning and operations phases:
\begin{itemize}
\item monitoring of the Front-End Motherboards
\item individual LArTPC channel signal processing (noise reduction, deconvolution)
\item basic event visualization in 2D projections
\item initial data preparation for an advanced 3D event display running on a separate remotesystem
\item liquid Argon purity monitoring using cosmic $\mu$ tracks
\item estimation of the number of hits and charge collected in each section of the LArTPC
\item estimation of signal-to-noise ratio
\end{itemize}

%\noindent For a few of these metrics the system produces time-series plots in addition to the tabulated data.
%The DQM applications are often called \textit{payload jobs} so as to distinguish them from service and
%infrastructure jobs which exist to support the function of the overall system.


\section{Design of the p3s and DQM services}


The  p3s  allows the experiment to benefit from combination
of high capacity, high performance distributed storage (CERN EOS), CERN networking
resources and its Tier-0 computing facility. It utilizes  the pilot-based approach \cite{eps}
to workload management characteristic of such well-known systems as PanDA and
DIRAC \cite{panda,dirac} and a Web interface which provides excellent job monitoring capabilities.
The DQM jobs running within the p3s framework generate output which is made
accessible to the users by means of the Web-based DQM Content Service.

Both the p3s server and the DQM content server are implemented as Django
Web applications \cite{django}. While the p3s instance deployed for \pd-SP is configured to use
the resources provided by CERN it can in fact manage the resources of any cluster,
local or remote and has been succesfully tested in these scenarios as well.

\section{The Self-Describing DQM Output}

Due to dynamic nature of application development in the context of the \pd beam
test it is not practical to create static menus, links or make many assumptions about
the categories and content of plots and other data products coming out of the DQM
application. Instead,
each DQM application is expected to produce two JSON files in compliance
with simple templates. One file is referred to as \textit{summary} and it contains basic
information about the attributes of the raw data file used as input. The second JSON file
referred to as \textit{description} provides references to plots produced in formats such
as PNG, as well as description of \textit{categories} and \textit{sub-categories} which
are \textbf{dynamically} translated by the server into
groups of menus with links necessary to access these plots on the dynamically generated
web pages. This approach greatly simplifies integration of different types of payload jobs 
and data into the DQM content service, with minimal or no changes to the
server code.


A few types of data in DQM can be accessed more efficiently if they are presented as time
series plots. To handle such cases a web page template was created containing
Javascript code making use of Google Charts functionality, so that the time series
plots are generated on the fly at the time of the user's request, based on the content
of the DQM database.

\section{Deployment, Testing and Operations}

The code of p3s and the DQM content server is version-controlled using \textit{git}
and GitHub. The p3s and DQM web services as well as their back-end database server have been deployed on
virtual machines of the CERN OpenStack cloud. Processes that need to be run periodically,
\textit{i.e.}\,refreshing the p3s pilot job population, automatic DQM job generation based on detection
of fresh raw data written to EOS \textit{etc.}\,are maintained using \textit{acrontab} which is a kerberized
distributed version of crontab used at the \textit{lxplus} interactive Linux facility at CERN.

The system was tested in two data challenges preceding the data taking period, running
at data rates and job count exceeding the expected operation parameters by roughly
a factor of two. During this testing and actual operation the p3s proved exceptionally stable \cite{chep18},
with infrequent failures due to outages of some elements of the local infrastructure
(e.g.\,storage and the batch facility) but not the system itself.

\section{Prompt Processing and DQM Design for DUNE}

The concept and design of the \pd-SP prompt processing system and the DQM content
service have been successful in helping the experiment to achieve its goals. The system
was of great value to the team commissioning and operating the detector. It can certainly
be expected that similar functionality will be required during the commissioning
and operation of the full-scale DUNE LArTPC. The following design features proven in
\pd-SP may be incorporated:
\begin{itemize}
\item Clean separation of DQM monitoring and prompt processing system, designed, deployed and operated as two independent entities

\item The ability of the prompt processing system to utilize any available computing
platform (clusters, data centers, cloud resources etc)

\item The self-describing data produced by DQM applications which allows for dynamic
behavior if of the DQM Content Web Service and quick turnaround cycle for new and
modified applications

\item Technology choices that favor well established, industry standard and easy to maintain
platforms

\end{itemize}

\noindent In addition to the above, given the anticipated long-term operation of DUNE, the following
may be of value when implementing the prompt processing and DQM for the full experiment:

\begin{itemize}

\item Container-based deployment model for Web services

\item Web tools to aid developers in creation of data descriptors for integration of applications into DQM

\item Given the scale of the detector, adequate mapping services which would facilitate correlation
of specific plots and metrics to groups of channels, electronics module, sections of the TPC etc.

\end{itemize}



%For tables use syntax in table~\ref{tab-1}.
%\begin{table}
%\centering
%\caption{Please write your table caption here}
%\label{tab-1}       % Give a unique label
% For LaTeX tables you can use
%\begin{tabular}{lll}
%\hline
%first & second & third  \\\hline
%number & number & number \\
%number & number & number \\\hline
%\end{tabular}
% Or use
%\vspace*{5cm}  % with the correct table height
%\end{table}
%
% BibTeX or Biber users please use (the style is already called in the class, ensure that the "woc.bst" style is in your local directory)
% \bibliography{name or your bibliography database}
%
% Non-BibTeX users please use
%


\begin{thebibliography}{99}

\bibitem{eps} M.Potekhin et al. \emph{The protoDUNE-SP experiment and its prompt
processing system}. Proceedings of Science (EPS-HEP2017) 513

\bibitem{panda}
T. Maeno et al. \emph{Overview of ATLAS PanDA Workload Management.~J. Phys.: Conf. Series.} Vol.\textbf{331}. IOP Publishing, 2011,
doi:10.1088/1742-6596/331/7/072024


\bibitem{dirac}
A. Casajus et al.  \emph{DIRAC Pilot Framework and the DIRAC
Workload Management System.~J. Phys.: Conf. Series.} Vol.\textbf{219}. IOP Publishing, 2010,
doi:10.1088/1742-6596/219/6/062049

\bibitem{django}
N. George \emph{Mastering Django: Core. The Complete Guide to Django 1.8 LTS}~ GNW Independent Publishing, ISBN: 099461683X

\bibitem{chep18} M.Potekhin et al.  \emph{The Prompt Processing System and Data Quality Monitoring in the protoDUNE-SP Experiment}, FNAL DocDB 14137.

\end{thebibliography}


\end{document}
