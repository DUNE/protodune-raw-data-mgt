\documentclass[pdftex,12pt,letter]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{cite}
\usepackage{url}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}


\newcommand{\pd}{protoDUNE\xspace}
\newcommand{\filesize}{8\,GB\xspace}
\begin{document}
%
\title{Supporting Materials for the DUNE TDR Computing sections on the Prompt Processing System and Data Quality Monitoring in \pd-SP and DUNE in general}


\author{M.\,Potekhin}

%
\maketitle
%
\section{Motivations for p3s and DQM}
\label{sec:motivations}
%The DUNE Collaboration has developed and utilized a prompt processing system to support
%the Data Quality Monitoring activities crucial to the commissioning and operations of
%\pd-SP.


The \pd-SP Data Acquisition (DAQ) is equipped with a capable online monitoring system
which receives data via the network and does processing in real time.
However, certain types of processing required for Data Quality Monitoring (DQM), 
require more resources than are available within the DAQ footprint, and include jobs
that take substantially longer time than processes run in the online monitor.
That could present a resource and data management problem for the DAQ and
intefere with the crucial DAQ function. We note that it is necessary to insulate the DAQ
from frequent software updates of the DQM software dictated by the dynamic nature comissioning
and operations of the experiment. 
For these reasons a separate \textit{\pd Prompt Processing System} (abbreviated as p3s) was put
in place  with the following characteristics \cite{eps}:
\begin{itemize} 

\item turnaround time on the scale of minutes (in limited cases tens of minutes)

\item processing a small fraction of events compared  to the online monitor
but in more detail and with a variety of methods

\item extensive and scalable computing resources provided by the CERN batch facility

\item accessing data in CERN central storage, with no direct coupling to the DAQ system

\item considerable flexibility in introducing, modifying and configuring software 

\item substantial storage and browsing capability for accessing the DQM results

\end{itemize}


\section{The DQM Content}
The following types of DQM applications were used at various times during
commissioning and operations phases:
\begin{itemize}
\item monitoring of the Front-End Motherboards
\item individual LArTPC channel signal processing (noise reduction, deconvolution)
\item basic event visualization in 2D projections
\item initial data preparation for an advanced 3D event display running on a separate remotesystem
\item liquid Argon purity monitoring using cosmic $\mu$ tracks
\item estimation of the number of hits and charge collected in each section of the LArTPC
\item estimation of signal-to-noise ratio
\end{itemize}

%\noindent For a few of these metrics the system produces time-series plots in addition to the tabulated data.
%The DQM applications are often called \textit{payload jobs} so as to distinguish them from service and
%infrastructure jobs which exist to support the function of the overall system.


section{Design of the p3s and DQM services}


The  p3s  allows the experiment to benefit from combination
of high capacity, high performance distributed storage (CERN EOS), CERN networking
resources and its Tier-0 computing facility. It utilizes  the pilot-based approach \cite{eps}
to workload management characteristic of such well-known systems as PanDA and
DIRAC \cite{panda,dirac} and a web interface which provides excellent job monitoring capabilities.


\subsection{Design of the DQM Content Service}
The results produced by the payload jobs managed by p3s can only be useful if there is an
efficient interface for the users to access these derived data. Such interface is provided in \pd-SP
by a web service accessible through a web browser and a few \textit{CLI} (command line interface) utilities.
The DQM output (the ``content'') can be stored in the system in two ways:
\begin{itemize}

\item when the basic unit of data that needs to be stored is a file, e.g.\,an image in the PNG format, such
file is stored on disk (EOS) and its location is recorded in a database so it can be later retrieved
and accessed through a dynamically generated web page,  based on some selection criteria; 
this is simplified by the fact that the EOS storage is directly accessible from the Apache server
used for this purpose

\item when the basic unit of data is an array of numbers these are for the most part stored directly
in the database and are used in dynamically generated web pages either in tabulated format or
to generate graphs by using JavaScript

\end{itemize}



\section{Technologies and Interfaces}
\subsection{Web Services}

Both the p3s server and the DQM content server are implemented as Django
\cite{django} applications written in Python with standard components such
as the Apache web server and PostgreSQL RDBMS as the back-end storage
(a few other RDBMS can be used as well). All interactions with either service
are conducted via HTTP either from a web browser or one of the included \textit{CLI} utilities.
While the p3s instance deployed for \pd-SP is configured to use
the resources provided by CERN the system itself is platform-agnostic in the sense that
it can manage the resources of any cluster or group of worker nodes and in fact has
been succesfully tested in these scenarios as well.

The p3s server is responsible for workload management by matching available pilots to the job requests,
and also provides monitoring capabilities by allowing the user to browse and navigate
tabulated data describing the state of the various objects in the system (e.g. pilots, jobs, data files \textit{etc.}).
The design of the monitoring pages leverages the Django web page template functionality and
the well known \textit{django-tables2} package which results
in a very small amout of application code. An example of pages served by p3s is
presented in Figure \ref{fig:p3s_dash} (the p3s dashboard).


 A suite of CLI clients is provided as a component of p3s for managing pilots, jobs and
other entities. In case interaction with the server requires exhange of messages these are
formatted as JSON, pimarily because it is trivial to parse it in Python.

There is a centralized log of critical system messages stored in the p3s database which
is used to troubleshoot the system. It is also used to automatically generate alarms
for the operators should certain parameters fall out of their nominal range.

\subsection{The Content Server Interface: the Self-Describing Data}
Due to highly dynamic nature of application development in the context of the \pd beam
test it is not practical to create static menus, links or make many assumptions about
the categories and content of plots and other data products coming out of the DQM
application. Any such information or logic included in the server code in a static manner
would result in frequent and time-consuming updates of the server code. Instead,
each DQM application is expected to produce two or more files in accordance
with simple JSON schemas. One file is referred to as \textit{summary} and it contains basic
information about the attributes of the raw data file used as input, such as the run number
and a few other indices. This file is stored as a JSON string in the back-end database
of the DQM server. It may also optionally contain a number of summary metrics
that are then parsed by the page-generating logic of the server and presented
in the DQM tables.

The second JSON file (or a group of files) referred to as \textit{description} or \textit{file list}
provides references to plots produced in formats such as PNG, as well as description of
\textit{categories} and \textit{sub-categories} which are automatically translated by the server into
groups of menus with links necessary to access these plots on the dynamically generated
web pages. An example of such menus is presented in Figure \ref{fig:tpc_monitor}.
This approach greatly simplifies integration of different types of payload jobs 
and data into the DQM content service, with minimal or no changes to the
server code.


A few types of data in DQM can be accessed more efficiently if they are presented as time
series plots. To handle such cases a web page template was created containing
Javascript code making use of Google Charts functionality, so that the time series
plots are generated on the fly at the time of the user's request, based on the content
of the DQM database.

\subsection{Deployment, Testing and Operations}

The code of p3s and the DQM content server is version-controlled using \textit{git}
and GitHub. The p3s and DQM web services as well as their back-end database server have been deployed on
virtual machines of the CERN OpenStack cloud. Processes that need to be run periodically,
\textit{i.e.}\,refreshing the p3s pilot job population, automatic DQM job generation based on detection
of fresh raw data written to EOS \textit{etc.}\,are maintained using \textit{acrontab} which is a kerberized
distributed version of crontab used at the \textit{lxplus} interactive Linux facility at CERN.

The system was tested in two data challenges preceding the data taking period, running
at data rates and job count exceeding the expected operation parameters by roughly
a factor of two. During this testing and actual operation the p3s proved exceptionally stable,
with infrequent failures due to outages of some elements of the local infrastructure
(e.g.\,storage and the batch facility) but not the system itself.

\section{Conclusions}

The \pd-SP experiment running at CERN in 2018-2019 requires adequate Data
Quality Monitoring. To meet these requirements the DUNE Collaboration has developed
and deployed web services which manage the execution of the DQM jobs on the CERN
facilities and dynamic handling of the DQM data content with efficient user interfaces.
The design was based on existing and proven technologies such as Django
for the web applicaiton framework,  pilot jobs for efficient interface to
the batch system, standard  JavaScript libraries and a number of others. The DQM software
running on the \pd-SP prompt processing system was  successfully used during
the commissioning phase of the experiment and will remain an imporant part
of its ongoing operation.

%For tables use syntax in table~\ref{tab-1}.
%\begin{table}
%\centering
%\caption{Please write your table caption here}
%\label{tab-1}       % Give a unique label
% For LaTeX tables you can use
%\begin{tabular}{lll}
%\hline
%first & second & third  \\\hline
%number & number & number \\
%number & number & number \\\hline
%\end{tabular}
% Or use
%\vspace*{5cm}  % with the correct table height
%\end{table}
%
% BibTeX or Biber users please use (the style is already called in the class, ensure that the "woc.bst" style is in your local directory)
% \bibliography{name or your bibliography database}
%
% Non-BibTeX users please use
%


\begin{thebibliography}{99}

\bibitem{eps} M.Potekhin et al. \emph{The protoDUNE-SP experiment and its prompt
processing system}. Proceedings of Science (EPS-HEP2017) 513

\bibitem{panda}
T. Maeno et al. \emph{Overview of ATLAS PanDA Workload Management.~J. Phys.: Conf. Series.} Vol.\textbf{331}. IOP Publishing, 2011,
doi:10.1088/1742-6596/331/7/072024


\bibitem{dirac}
A. Casajus et al.  \emph{DIRAC Pilot Framework and the DIRAC
Workload Management System.~J. Phys.: Conf. Series.} Vol.\textbf{219}. IOP Publishing, 2010,
doi:10.1088/1742-6596/219/6/062049

\bibitem{django}
N. George \emph{Mastering Django: Core. The Complete Guide to Django 1.8 LTS}~ GNW Independent Publishing, ISBN: 099461683X



\end{thebibliography}


\end{document}
