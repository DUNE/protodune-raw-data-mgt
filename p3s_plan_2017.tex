\documentclass[pdftex,12pt,letter]{article}
\usepackage[binary-units=true]{siunitx}
\usepackage[margin=0.75in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}
\usepackage{xspace}
\usepackage{amssymb}

%\usepackage[firstpage]{draftwatermark}


\bibliographystyle{unsrt}

\newcommand{\fixme}[1]{\textbf{FIXME: #1}}    
\newcommand{\pd}{protoDUNE\xspace}

\title{Planning of the \pd Prompt Processing System}
\date{\today}
\author{M.Potekhin, B. Viren}

\begin{document}
%\SetWatermarkText{DRAFT}
%\SetWatermarkLightness{0.9}
%\SetWatermarkScale{3}

\maketitle

\begin{abstract}
\noindent The purpose of this document is to inform the leadership
of the Single-Phase \pd experiment (NA04) about the scope, deliverables,
schedules, interfaces and other crucial characteristics of the prompt processing
system (p3s).
\end{abstract}

% \tableofcontents

\pagebreak

\section{Overview}
We will use the name ``p3s'' to refer to the \textit{\pd Prompt Processing System.}
The primary purpose of p3s is to support \textit{Data Quality Monitoring} (DQM) in \pd.
The following Ñˆtems will be covered in this document:
\begin{itemize}
\item p3s scope and deliverables
\item minimum required capabilities and functionality of p3s at key points in time 

\item a set of milestones in about 2 months time increments, including relevant testing

\item resource assessment including the effort profile in FTEs and people assigned to the task

\item outline of risk assessment 
\end{itemize}


\section{The scope of p3s}
p3s is a platform for management of automated DQM/prompt processing streams
in \pd. As a work item, it is an infrastructure element and does not include the actual software for
computational payloads --- this is the responsibility of the DAQ, Reco and other working groups with interest in producing prompt DQM results.

\subsection{The p3s data input and output}
\label{sec:io}

From the point of view of raw data flow, the boundary between online and offline scope is the Online Buffer which is a storage system attached to DAQ computers.
It exists primarily to assure continued DAQ operation in the event of an outage of the link to the CERN network.
The ``\pd/SP Data Scenarios'' spreadsheet~\cite{docdb1086}
describes expected running conditions and provides estimates
of data volumes and rates relevant to this boundary.

All raw files written by the DAQ to the buffer will be transferred to CERN EOS
using the Fermilab file transfer system (F-FTS) \cite{docdb1212,fts}.
F-FTS takes responsibility for purging any and all files it is given and can do so in a configurable manner.

The prompt processing DQM jobs will ingest a small percentage ($\lesssim$1\%) of raw data files.
The files are assumed to be served via XRootD~\cite{xrootd}.
This allows flexibility in ingesting files from one or both of:
\begin{itemize}
\item the Online Buffer
\item CERN EOS~\cite{eos}
\end{itemize}
Ingesting can be done by staging entire files locate disk via an agent provided by p3s.
Alternatively, jobs which require only a portion of data in one file may directly access that via XRootD.

A p3s processing may be composed of a number of jobs.  
Jobs may communicate via intermediate files which are managed by p3s.
Final output files produce by these jobs are then presented to end users by p3s in a number of ways described below.
The final output files can be categorized in the following way

\begin{description}
\item[graphics] files in PNG or SVG format representing histograms, plots, event displays, etc.
\item[summary] files in JSON format provides ancillary information for graphics files including captions or other information that may be directly rendered by a user interface.
\item[reference] files which should be retained for browsing by the user and which may include logs or intermediate results. 
\end{description}

\noindent The two end-user interfaces that p3s will provide are:

\begin{description}
\item[web pages] a web server which allows browsing of all results, and include features such as refreshing of specific time critical results
\item[notifications] a system that interprets \textit{summary} files to provide alarm notification via email, mobile push, or other means.
\end{description}

As needed, some portion of the files produced by p3s jobs will be
archived to mass storage and made available for distribution to the
collaboration.


\subsection{p3s and the Online Monitoring as complementary systems}
The p3s fills the gap between the very low latency Online Monitoring (OM)
system which is engineered as a component of DAQ on one hand, and
the general offline processing on the other. Conceptually, they both serve
the same purpose of generating time-critical DQM information on a short time
scale needed to ascertain the condition and performance of both the detector
and the DAQ and are complementary to each other. 
The benchmark turnaround time for p3s jobs 
according to the requirements set forth in a separate document 
\cite{docdb1811}  is set at 10\,min for reference purposes,
of course there will be several jobs types and execution times will
vary for many reasons. This time scale fulfills the need to have \textit{actionable}
DQM information in time for operators to take action and prevent loss
of useable data and/or valuable beam time.
\ \\
\\
\noindent By having p3s as a separate 
component in addition to the OM the following issues are adressed:
\begin{itemize}

\item \textbf{scalability}: DAQ and OM operate in a purpose-built computing environment characterized
by an extremely high bandwidth and a relatively limited CPU budget, as dictated by the
primary mission of these systems. Scaling up computational capabilities of DAQ
is still possible but likely to be prohibitevely expensive due to nature and cost of
the hardware used in DAQ. The prompt processing system from the very beginning
was conceptualized as built upon common (and in fact recycled) general purpose hardware
with ample CPU and somewhat limited network bandwidth. This makes scaling out of
the system very economical and even allows for inclusion of opportunistic resources
with very few requirements to be met.

\item \textbf{test of data integrity}: OM will consume data in the form of network streams generated
by the DAQ specifically for that purpose, i.e.~OM won't operate on actual \textit{files} generated
by the DAQ. This achieves a degree of simplicity of data handling and very low latency
in distributing the data to CPU, but also precludes the possibility to ascertain that the data
in its final form (i.e.~as finalized files on disk prepared for sinking to mass storage) is indeed
formatted correctly and is otherwise valid in that shape. The p3s whose input is precisely the finished
files coming out of DAQ serves in that role in addition to others.

\item \textbf{coupling/dependency}: 
  The running of p3s jobs is almost entirely independent from the operation of DAQ.
  The interface between the two consists of notification of the readiness of a potential raw file to be ingested and for p3s jobs to know how to read and unpack it.  This interface, once established, is expected to be relatively stable.
  Payload jobs run by p3s can be quickly modified
and adapted to produce new types of results during \pd commissioning and
data taking periods without impacting operation of DAQ.

%%%% (bv) I don't really think this is a distinction.  why can't 100 users browse OM results?
%%%%
%% \item \textbf{access to data and monitoring software}:
%% It is expected that access to DAQ, OM and monitoring data generated in the latter will be
%% restricted to a very limited number of people to reduce operational risks 
%% (e.g.~it is undesirable to have a hundred users browsing histograms coming out of OM
%% or introduce new algorithms into the online monitoring). Being more scalable and 
%% decoupled from the core DAQ the p3s will allow to make visual and data products
%% more widely accessible to the Collaboration and enable more users to contribute
%% to creating and modifying payloads ``in situ'' as the experiment progresses.


\end{itemize}



\subsection{Categories of Jobs in p3s}
\label{sec:categories}

The exact makeup of the p3s payload jobs is to be determined by Reco,
DAQ and other groups which have data quality monitoring needs.  It is
expected these jobs to fall into the following categories which are
described more fully in \cite{docdb1811}.  They are progressive and,
for now, focused on TPC data:

\begin{description}

\item[DAQ] category consists of results requiring no data
  decompression and likely will be produced in OM and by p3s jobs.  It
  might includes summaries of things like (compressed) fragment sizes,
  error flags, data rates.

\item[ADC] results require data decompression but no other substantial
  CPU and will consist of summary of ADC-level data.  Examples include
  mean/RMS values over time in various groupings and level of detail
  (ASIC, FEMB, RCE, APA etc).

\item[FFT] category consists of the application of discrete Fourier
  transforms (FFT) to ADC-level data primarily to understand any
  excess noise and its possible evolution.

\item[SIG] includes ADC mitigation, removal of any excess noise and
  the deconvolution of detector response functions in order to
  understand the signals related to actual activity in the detector.

\item[RECO] category includes application of any high-CPU algorithms
  for imaging and reconstructing the topology of activity in the
  detector and may include high level semantic event classifications.

\end{description}

\subsection{The Scope of Hardware}
P3s will require hardware to run its Web and database servers
and to provide enough storage for the final output files as described in
section~\ref{sec:io}.  It is expected that this hardware will be provided
by the CERN Neutrino Platform either from the pool of the machines
which beloong to the existing~\cite{neut} cluster, or procured separately
if performance and scalability tests indicate such necessity.


The storage required for the output files is not currently known.  It
depends on the catalog of payload jobs that will be run and the desire
of the collaboration for the retention of the results.  The largest
single type of result will be event displays.  They will an amount of
storage similar to the raw data from which they are produced.  If one
event display per minute is produced and retained indefinitely some
few tens of TB will be required.  The remanding class of results will
require far less storage.

The hardware required to run the payload jobs themselves of course
depends on the nature of their CPU needs and the fraction of triggers
they consume.  P3s follows a \textit{pilot}-based paradigm and so is
able to provide a uniform execution environment to its payload jobs
over a variety of native hosting environments.  This gives p3s the ability
to scale elastically across different hosts (oor faciltieis) as needed.  Initial expectation
is that DQM jobs will be run by p3s on nodes in the \textit{neutdqm}
partition of \textit{neut} cluster.  Other facilities such as
\textit{lxbatch} \cite{lxbatch} may be added if needed and as available.


\section{Required Capabilities}
\label{sec:capabilities}

The p3s must have capabilties to support categories of processing outlined
in Sec.\,\ref{sec:categories}. The term ``task'' is used here to designate a set
of related jobs, for a example a processing chain where each job consumes
data produced by its predecessor. Jobs can be used to add a wide range
of functionalty to the system, e.g.~copy a fraction of data to mass storage
if necessary or produce a set of PNG files based on intermediate ROOT files,
or perhaps generate an alarm if a certain metric falls out of prescribed range.

The p3s supports description of tasks as DAGs and also supports task templates
for ease of operation. The first job in the chain reads data from
the Online Buffer or EOS. In the limiting case, a task may contain just one payload
job; there is no set limit on the number of jobs in the DAG or its topology.
Submission of individual \textit{ad hoc} jobs by users is also supported.

\begin{description}
\item[Job and Task Management]\ 
\begin{itemize}

\item automatic generation of tasks upon arrival of fresh data to the buffer

\item distribution of jobs to processing resources assigned to p3s

\item management of tasks i.e.~orchestration of execution of jobs within
a task according to the dependencies between jobs

\item prioritization of tasks and jobs (during assignmenet to processing slots)
 including automated dynamic changes of priorities in order to ensure completion
of critical tasks

\end{itemize} 

\item[User and System interfaces]\ 
\begin{itemize}

\item full remote control of the system by operators via appropriate interfaces

\item interface to the Online Buffer and EOS in order to access the input data

\item interface to F-FTS necessary to move a portion of p3s output to external
sites and mass storage

\item Web interface for task monitoring and debugging of p3s


\end{itemize} 

\item[Web Access to Results]\ 
\begin{itemize}

\item the data produced by p3s will be made available to the users via a Web service
(e.g. a Web page populated with plots selected according to user-specified criteria)

\item for the data which is preserved as data files (as opposed to visual products)
there will be a catalog accessible via a browser/Web page

\end{itemize} 

\item[Notification of Exceptions]\ 
\begin{itemize}

\item summary files will be interpreted and checked for fields holding quantities which are to subject to limits defined in p3s.

\item when a quantity is found to be outside of set limits p3s sends notification to registered individuals. 

\item quantity-experts will have ability to modify the list of watched quantities and their limits.

\end{itemize} 

\end{description}

\section{Components and Deliverables}

The core of p3s is a Django-based \cite{django} application written in Python. The
deliverables in the systems category are as follows:

\begin{description}
\item[p3s server] --- a Web service which supports the capabilities described in Sec.~\ref{sec:capabilities}

\item[user tools] --- such as CLI clients necessary to manipulate the state of the system, submit
and manage tasks in the manual mode if necessary, and to perform general management and
maintenance of the system

\item[data server] --- a Web/network service which makes visual and data products produced by p3s
available to the users

\item[data network access] --- a XRootD-based service continuously running on the DQM cluster
and communicating with small-footprint XRootD service running on the Online Buffer, and also EOS.

\item[Web and DB servers] --- needed as a platform to support the items listed above. These
can be based on a variety of products and offer a degree of interoperability.

\end{description}

\noindent 

\section{Timeline and Milestones}

The items below include system deployment, functional and integration testing.
Critical milestones are marked by bullets.

\begin{description}

\item[March'17: p3s dev server] --- a continuously running instance of p3s deployed on the DQM cluster,
utilizing the ``Django development server'' and \textit{sqlite} database back-end. LArSoft payloads as
test case. Visual and data products served from the same server.

\item[April'17: XRootD] --- a continuously running XRootD service deployed on the DQM cluster with
fully functional interface to EOS and ongoing test transfers.

\item[June'17: full chain] --- an instance of p3s running on dev server/sqlite with automated
generation of workflows
\begin{itemize}
\item DAQ vertical slice readiness
\end{itemize}

\item[July'17: Web Service for Visual and Data Products] --- an instance of a Web server optimized
for delivery of visual and data products produced by p3s.

\item[August'17] --- p3s migration to Apache and PostgreSQL. Integration testing with the
DAQ Online Buffer.

\item[September'17] --- p3s/DAQ integration testing
\begin{itemize}
\item Full DAQ test readiness
\end{itemize}

\item[Q4'17] --- p3s stress and scalability testing, additional development

\item[Q1'18] --- continuous p3s operation with realistic workloads, running
MC/Reco jobs in utility mode

\item[Q2'18]\ 
\begin{itemize}
\item Full data taking readiness
\end{itemize}

\end{description}


\section{Resources}

\subsection{Effort Levels}

Development/engineering
\begin{itemize}
\item M.Potekhin (BNL) --- 100\%\,FTE
\item B.Viren (BNL) --- 10\%\,FTE
\end{itemize}

\noindent System administration and support (working assumption)

\begin{itemize}
\item N.Benekos (CERN) --- 50\%\,FTE (TBD)
\item G.Savage (FNAL) --- 50\%\,FTE (TBD)
\end{itemize}

\subsection{Mat\'eriel}
To be useful, p3s needs O(100) cores during its operation. As noted above,
an additional small number of adequately configured machines are needed for
deployment of Web and DB servers to support the p3s function.

It is a working assumption at the time of writing that a significant portion of
the \textit{neut} cluster \cite{neut} will be assigned for p3s use as \pd ramps up
its operations in 2017 and into 2018.


\section{Risk Assessment}

Technologies used in p3s are not new, they are mature and tested in the field, which reduces
overall implementation risk. The remaining risk factors are
\begin{itemize}

\item insufficient allocation of hardware to p3s i.e.~failure to assign a sufficient number
of \textit{neut} cores to p3s and/or provide sufficiently capable machines to support Web
and DB functionality

\item lack of coordination across DAQ/DQM teams which may delay functional and integration
testing

\item late F-FTS deployment which will delay functional and integration testing of p3s access
to data in EOS and usage of mass storage for its output data

\item lack of manpower to provide robust sysadmin and other crucial support for the DQM
cluster and its core services (Web/DB)

\end{itemize}

\clearpage
\begin{thebibliography}{1}

\bibitem{docdb1086}
{DUNE DocDB 1086: \textit{ protoDUNE/SP data scenarios with full stream (spreadsheet)}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1086}




\bibitem{docdb1212}
{DUNE DocDB 1212: \textit{Design of the Data Management System for the protoDUNE Experiment}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1212}

\bibitem{fts}
{The Fermilab File Transfer System}\\
\url{http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=5412&filename=datamanagement-changeprocedures.pdf&version=1}


\bibitem{xrootd}
{XRootD}\\
\url{http://www.xrootd.org}

\bibitem{eos}
{The CERN Exabyte Scale Storage}\\
\url{http://information-technology.web.cern.ch/services/eos-service}



\bibitem{docdb1811}
{DUNE DocDB 1811: \textit{Prompt Processing System Requirements for the Single-Phase protoDUNE}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1811}



\bibitem{neut}
{Neutrino Computing Cluster at CERN}\\
\url{https://twiki.cern.ch/twiki/bin/view/CENF/NeutrinoClusterCERN}

\bibitem{lxbatch}
{The CERN batch computing service}\\
\url{http://information-technology.web.cern.ch/services/batch}

\bibitem{django}
{Django}\\
\url{https://docs.djangoproject.com/en/1.10/}


\end{thebibliography}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
