\documentclass[pdftex,12pt,letter]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{cite}
\usepackage{url}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}

\newcommand{\pd}{protoDUNE\xspace}
\newcommand{\xrd}{XRootD\xspace}

\title{The clustered storage option for the protoDUNE NP04 Online Buffer}
\date{\today}
\author{N. Benekos, M. Potekhin and B. Viren}


\begin{document}
\maketitle

\begin{abstract}
\noindent  This note describes the clustered storage
solution for the online buffer of the \pd (experiment NP04, Single-Phase LArTPC).
Basic data characteristics and  parameters of such storage are estimated. \xrd is proposed as the underlying
storage clustering technology. It is suggested that among other options, a portion of the existing   \textit{Neutrino Platform}
computer cluster at CERN could potentially be utilized for the development, testing and actual implementation of the NP04 online buffer. 
The reader is encouraged to peruse references to DUNE DocDB provided in the text for more detailed information on individual
items included in this note.
\end{abstract}

%%%%%%%%%%%%%
\section{Overview}
\subsection{The Role of the Online Buffer}
\label{sec:the_role}
The online buffer of the NP04 experiment must be put in place to absorb the potentially high instantaneous (in-spill) DAQ
data rate before transmission of raw data to mass storage. It is also needed to satisfy the CERN requirement of providing
3 days worth of storage to make operation of the experiment possible in case of a network and/or central services outage.
Combined with the projected data rate, this requirement determines the overall capacity of the buffer. From the buffer,
the data needs to be delivered to the high-performance  disk storage (EOS) located at the CERN central services.
The place of the buffer in the data transmission chain therefore can be visualized as in Fig.\,\ref{fig:big-picture}.
\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/big-picture.pdf}
  \caption{Place of the buffer in the data transmission chain.}
  \label{fig:big-picture}
\end{figure}

\subsection{Data Characteristics}

The current estimates \cite{docdb1086} bracket the expected range of data rate
and data volume in \pd \cite{docdb186} due to in-spill beam triggers and out-of-spill cosmic ray muon triggers.
For historical reasons, the scenarios on the opposite sides of the bracket are named ``Central'' and ``High rate''.
There are two main assumptions which drive the expected data rate:
\begin{itemize}
\item the beam trigger rate as per the run plan
\item the requirement that a cosmic-ray muon trigger is acquired out-of-spill for every in-spill beam trigger; this is due to
the calibration plan currently in the works and whose details are out of the scope of this document and subject to
further R\&D
\end{itemize}
\noindent Below is the summary of the principal data characteristics in \pd:
\begin{table}[tbh]
\centering
\begin{tabular}{l l}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
\hline
trigger rate				& 25 -- 50 Hz \\  \hline
peak data rate (DAQ internal, instantaneous during spill)	& 1.5 -- 3.0 GB/s \\ \hline
daily data volume &  25 -- 50TB \\ \hline
3-day buffer capacity & 150 -- 300TB \\  \hline
\hline
\end{tabular}
\caption{\label{tab:data_char}Expected protoDUNE data characteristics.}
\end{table}

\subsection{F-FTS}
Basic reqruirements for the system to handle the \pd raw data are presented in \cite{docdb1209}.
The design which meets these requirements is outlined in \cite{docdb1212}. It leverages the
Fermi File Transfer System (F-FTS) to perform two essential transfers:
\begin{itemize}
\item from the online buffer to CERN mass disk storage (EOS)
\item from EOS to CERN tape (CASTOR) and  mass storage at FNAL and other US sites
\end{itemize}

\noindent It is foreseen that two distinct instances of F-FTS will be deployed to fill each
respective role. This is schematically illustrated in Fig.\,\ref{fig:ftsinstances}. The ``SP Disk
Buffer Farm'' in this diagram corresponds to the NP04 online buffer.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/ftsinstances_v2.png}
  \caption{The two instances of FTS used for marshaling raw data from protoDUNE.}
  \label{fig:ftsinstances}
\end{figure}

 \noindent F-FTS uses the Fermilab SAM system for file catalog and metadata functionality,
and to keep track of the state of each transfer. In order to make use of this functionality, metadata
needs to be created before F-FTS takes ownership of a particular file. Checksums also need to be
generated to validate transfers throughout the chain of transmission and are typically considered
a part of the file metadata. Computing the checksums
as well as collecting the necesary metadata require both I/O bandwidth and CPU.

Typically the F-FTS operates in the dropbox mode, whereby it detects the arrival of a new file
according to configurable rules (e.g. filename patterns). In this mode, it must have a POSIX-like access to the file
system which is under its watch. Alternatively, F-FTS can be triggered via a HTTP POST request
from an external agent. The actual transfer can be handled by F-FTS itself (i.e. it works
as a client) or by third-party tools. The state of the transfer is registered in SAM
and recovery mechanisms are available to ensure a robust transfer.

\section{Design of the Online Buffer}
\subsection{The Event Builders as the ``DAQ boundary''}
The Liquid Argon TPC  and a photon detector subsystem in \pd are read out by means of
computing elements called ``board readers''. Each reader gets data from a portion of the detector,
and each triggered event must be assembled separately from fragments of the data. This function
is performed by the Event Builders, as schematically shown in Fig.\,\ref{fig:upstream}.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/upstream.pdf}
  \caption{Conceptual diagram of the data flow within DAQ, from the detector to the Event Builders.}
  \label{fig:upstream}
\end{figure}
\noindent Orchestration of data flow from the Board Readers to Event Builders can be done in a number
of ways, and the current working assumption is that this will be implemented as a standard round-robin
configuration. More sophisticated load-balancing algorithms are alse being considered. As the name suggests,
the Event Builders create a complete data record in memory for each triggered event (readout period),
which then must be persisted in storage. It is optimal that  this storage also serves as the online buffer 
 which needs to be a part of the system (\ref{sec:the_role}), since otherwise an additional transfer proxedure
to a different storage device would need to be implemented and managed.

\subsection{Attached vs Clustered Storage}
Any \pd data taking scenario currently under consideration requires sustained rate
of ``data to disk'' in hundreds of MB per second. In practice this means that
multiple hard drives are needed to absorb such rate and have enough headroom
for stable operation of the system.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/doob-join.pdf}
  \caption{Conceptual diagram of the Event Builders interfacing the XRootD cluster.}
  \label{fig:doob-join}
\end{figure}

\section{Concept}

The \texttt{neut} cluster is now being formed using about 350 nodes in
total reclaimed from ATLAS.  We will dedicate about 50 nodes in
support of developing the disk buffer system for the single-phase
protoDUNE detector adequately scaled for storing three days worth of
expected data.  To label this sub-cluster we say
``\texttt{neut-spbuf}''.  Initially \texttt{neut-spbuf} will be for
developing and testing the buffer system design.  Meanwhile, we will
explore what is needed to migrate \texttt{neut-spbuf} into actual
operation.

\section{Disk}

The current \texttt{neut} nodes have very limited disk storage.  The
\texttt{neut-spbuf} nodes must be upgraded to provide storage to meet
the 3-day buffer requirement. To meet the ``High rate'' requirement we
will install $2\times 3$TB SATA disks in each of the 50
\texttt{neut-spbuf} nodes.

\section{Networking}

The ``High rate'' scenario requires sinking a peak of 3.0 GByte/sec
(24 Gbps) throughput during the beam spill.  Between spills, when
cosmic muon triggers are acquired, the throughput is somewhat reduced
but we take 3.0 GByte/sec as our requirement.  Spread across the 50
\texttt{neut-spbuf} nodes these streams this will approximately fill
50\% of the existing 1Gbps NICs.  We expect similar multiplicity at
the data production end (the Event Builder layer of the pD/SP DAQ).

During initial testing we will request a 20 Gbps link between the
current location of \texttt{neut}\footnote{CERN building 185} and
central CERN computing including EOS and the pD/SP detector
site\footnote{CERN building EHN1}.

To supply this connectivity we require 50 switch ports at 1Gbps and
(effectively) one switch port at 20Gbps.  Based on our current design
it is possible to segment the network streams so that the total
bandwidth is spread over multiple switches, for example two switches
each with 25 ports at 1Gbps and 1 port with 10 Gbps.  One example
switch is the Cisco SG500X-48P which can provide 48 1Gbps ports and
ample ports on the high-bandwidth side.  One such switch is needed on
the DAQ end of the 20Gbps link and one on the \texttt{neut-spbuf} end.

\begin{thebibliography}{1}
\bibitem{docdb1086}
{DUNE DocDB 1086: \textit{ protoDUNE/SP data scenarios with full stream (spreadsheet)}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1086}

\bibitem{docdb186}
{DUNE DocDB 186: \textit{ ProtoDUNE Proposal}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=186}


\bibitem{docdb1209}
{DUNE DocDB 1209: \textit{Basic Requirements for the protoDUNE Raw Data Mangement System}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1209}


\bibitem{docdb1212}
{DUNE DocDB 1212: \textit{Design of the Data Management System for the protoDUNE Experiment}}\\
\url{http://docs.dunescience.org:8080/cgi-bin/ShowDocument?docid=1212}



\bibitem{xrootd}
{XRootD, high performance, scalable fault tolerant access to data  repositories}.\\
  \url{http://xrootd.org/}.

\end{thebibliography}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
